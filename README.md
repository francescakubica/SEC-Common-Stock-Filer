# ğŸ“„ SEC Common Stock Filer â€” Streamlining Schedule 13D Analysis

![Deployed on Streamlit](https://img.shields.io/badge/Deployed-Streamlit-orange)
![Efficiency Boost](https://img.shields.io/badge/Productivity+Up-125%25-brightgreen)
![Powered by OpenAI](https://img.shields.io/badge/OpenAI-API-blue)

## ğŸ”— [Instructions & Demo Guide](https://www.canva.com/design/DAGb0qlASmU/TULDjcUx6GjvENKRvsaoeA/view?utm_content=DAGb0qlASmU&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h43ecb3e9d4)

---

## ğŸ“˜ Project Overview

This tool was developed as part of an applied research assistantship under **Professor Carmen Payne-Mann** at the **University of Southern California**. It automates the extraction of **common stock transaction data** from SEC **Schedule 13D filings**, significantly reducing the time required for manual data entry.

> ğŸ•’ Before: ~1 hour per 10 filings  
> âš¡ After: ~1 hour per 25 filings  
> **Result:** ~125% increase in efficiency

---

## ğŸ’¡ How It Works

Using the **OpenAI GPT model** via API, the application parses raw `.txt` Schedule 13D files and extracts relevant common stock transactions. Users upload individual text files, and the tool returns a clean `.csv` with the following columns:

- ğŸ“… Trade Date  
- ğŸ“ˆ Share Count  
- ğŸ’µ Buy/Sell Indicator  
- ğŸ’° Transaction Price

**Note:** The tool omits secondary columns (e.g., footnotes, indirect ownership) to maximize parsing accuracy.

---

## ğŸ‘¤ Designed for Research Assistants

This app is **not designed for bulk scraping**. Instead, it is intended to **supplement human analysis** by speeding up routine entry tasks, especially when working with SEC filings manually downloaded from EDGAR.

- ğŸ” **Upload files one at a time** for optimal accuracy.
- ğŸš« Avoid uploading filings **without transaction details**, as these will still generate a CSV with no rows, which may be confusing.

---

## âš™ï¸ Deployment & Architecture

This is my **first end-to-end deployed application** using GitHub and Streamlit:

- âœ… Front-end built in **Streamlit**
- ğŸš€ Hosted via **Streamlit Cloud**
- ğŸ§  Integrated with **OpenAI GPT API** for natural language parsing
- ğŸ“‚ Outputs downloadable CSVs for each file

Currently used by a team of **3 Research Assistants** at USC.

---

## ğŸ§ª Challenges & Solutions

### ğŸ” API Key Handling
**Challenge:**  
Allowing users to input and validate their own OpenAI API key securely.

**Solution:**  
Implemented `st.session_state` in Streamlit to persist the API key across user sessions. The "Validate" button runs a demo API call to check for authentication before storing the key in session.

---

### ğŸ“¦ Efficient DataFrame Handling
**Challenge:**  
Passing the main transaction DataFrame across multiple parsing steps without unnecessary copies.

**Solution:**  
Optimized performance by passing references to the original DataFrame rather than creating new instances. Reduced memory overhead and latency.

---

### ğŸ”— Front-End â†” Logic Integration
**Challenge:**  
Tying the upload/parse button functionality on Streamlitâ€™s UI to the inner logic parsing layer.

**Solution:**  
Modularized parsing logic, used callbacks, and ensured that the uploaded file content flows directly into the transaction pipeline and outputs a downloadable file.

---

## ğŸ“‚ Repository Structure

```bash
ğŸ“ __pycache__/                 # Compiled Python bytecode cache
ğŸ“„ .gitignore                  # Standard Git ignore rules
ğŸ“„ README.md                   # Project documentation
ğŸ“„ requirements.txt            # Python dependencies

ğŸ“„ frontend.py                 # Streamlit front-end (file uploader, UI)
ğŸ“„ processor.py                # Core GPT API logic and parsing
ğŸ“„ download_txt.py            # Handles download / return of final CSV
ğŸ“„ text_file_pipeline.py      # End-to-end file parsing pipeline and date formatting
